"""
Tests for "Specific Fact in Specific Chapter" Retrieval.

This is the critical requirement from the Bluejay Take-Home Interview:
> "I will ask about a specific fact in a specific chapter, so a proper RAG setup 
>  is essential â€” not just keyword search or summarization."

These tests verify:
1. Query normalization (voice-style numbers: "figure thirty seven" â†’ "figure 37")
2. Anchor detection (figure, table, section, chapter, page, reference)
3. CrossEncoder re-ranking for precision
4. Neighbor expansion for context
5. Structured citations in output
"""
import os
import sys
import logging
import time
from typing import Optional, List, Dict, Any

# Add backend to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# =============================================================================
# TEST DOCUMENT: Simulates a research paper with figures, tables, sections
# =============================================================================

RESEARCH_PAPER_TEXT = """
Abstract

This paper presents Video Models as Zero-Shot Learners, demonstrating that video 
foundation models can perform complex tasks without task-specific training. Our 
findings show significant improvements in text manipulation and visual reasoning.

1. Introduction

Large language models have revolutionized natural language processing. We extend 
this paradigm to video understanding through our proposed framework.

2. Related Work

Reference [15] discusses the foundation of video understanding models and their 
applications to robotics. Reference [23] explores multimodal learning approaches.

3. Methodology

3.1 Model Architecture

Our model uses a transformer-based architecture with 12 attention heads and 
768 hidden dimensions. Table 1 shows the model configurations.

Table 1: Model Configurations
| Config | Params | Layers | Hidden |
|--------|--------|--------|--------|
| Base   | 110M   | 12     | 768    |
| Large  | 340M   | 24     | 1024   |

3.2 Training Procedure

The model is trained on 10M video clips with self-supervised objectives.
Section 3.2.1 describes the data preprocessing steps in detail.

4. Experiments

4.1 Benchmark Results

We evaluate on three standard benchmarks. Table 2 compares our results.

4.2 Text Manipulation Success

Figure 37 shows the success rate for text manipulation tasks across different 
model sizes. The Base model achieves 85% accuracy while the Large model 
reaches 94% accuracy on the standard benchmark.

Figure 37: Text Manipulation Success Rate by Model Size
- Base Model: 85% accuracy
- Large Model: 94% accuracy  
- Our Method: 97% accuracy (state-of-the-art)

The improvement from 85% to 97% represents a 12 percentage point gain.

4.3 Video Understanding

Figure 38 presents temporal reasoning results. Our model outperforms prior 
work by 15% on the temporal benchmark. Table 3 provides detailed breakdowns.

5. Ablation Studies

Section 5.1 analyzes the contribution of each component:
- Attention mechanism: +8% improvement
- Temporal encoding: +5% improvement
- Multimodal fusion: +4% improvement

5.2 Error Analysis

We analyze failure cases on page 42 of the supplementary material. Common 
errors include temporal confusion (35%) and object tracking failures (25%).

6. Conclusion

We demonstrated that video models can serve as zero-shot learners. Reference [47] 
provides further theoretical analysis of this phenomenon. Equation 5 in the 
appendix formalizes the generalization bounds.

Appendix A

Equation 5: Generalization Bound
E[loss] â‰¤ train_loss + sqrt(complexity / n_samples)

This bound explains the sample efficiency observed in our experiments.
"""


# =============================================================================
# TEST UTILITIES
# =============================================================================

class TestResult:
    """Container for test results."""
    
    def __init__(self, name: str):
        self.name = name
        self.passed = False
        self.error: Optional[str] = None
        self.duration_ms: float = 0
        self.details: Dict[str, Any] = {}
    
    def __str__(self) -> str:
        status = "âœ… PASS" if self.passed else "âŒ FAIL"
        result = f"{status} {self.name} ({self.duration_ms:.1f}ms)"
        if self.error:
            result += f"\n   Error: {self.error}"
        if self.details:
            for key, val in self.details.items():
                result += f"\n   {key}: {val}"
        return result


class TestRunner:
    """Test runner with performance tracking."""
    
    def __init__(self):
        self.results: List[TestResult] = []
    
    def run_test(self, name: str, test_func):
        """Run a test function and record results."""
        result = TestResult(name)
        start = time.time()
        
        try:
            details = test_func()
            result.passed = True
            result.details = details or {}
        except AssertionError as e:
            result.error = str(e)
        except Exception as e:
            result.error = f"{type(e).__name__}: {e}"
        
        result.duration_ms = (time.time() - start) * 1000
        self.results.append(result)
        print(result)
        return result
    
    def summary(self) -> str:
        """Get test summary."""
        passed = sum(1 for r in self.results if r.passed)
        total = len(self.results)
        total_time = sum(r.duration_ms for r in self.results)
        
        return (
            f"\n{'='*70}\n"
            f"TEST SUMMARY: {passed}/{total} passed ({total_time:.0f}ms total)\n"
            f"{'='*70}"
        )


# =============================================================================
# UNIT TESTS: Query Enhancement
# =============================================================================

def test_query_enhancement_compound_numbers():
    """Test that compound numbers are normalized correctly."""
    from rag.retriever import DocumentRetriever
    
    test_cases = [
        ("figure thirty seven", "37"),
        ("section four point one", "4.1"),
        ("reference twenty three", "23"),
        ("chapter five", "5"),
        ("table forty two", "42"),
        ("page ninety nine", "99"),
    ]
    
    for query, expected_num in test_cases:
        enhanced = DocumentRetriever._enhance_query_for_keywords(query)
        assert expected_num in enhanced, \
            f"Expected '{expected_num}' in enhanced query for '{query}', got: {enhanced}"
    
    return {"queries_tested": len(test_cases)}


def test_query_enhancement_section_decimals():
    """Test section decimal patterns."""
    from rag.retriever import DocumentRetriever
    
    enhanced = DocumentRetriever._enhance_query_for_keywords(
        "What does section four point two say about text manipulation?"
    )
    
    # Should contain "4.2" after normalizing "four point two"
    assert "4.2" in enhanced or ("4" in enhanced and "2" in enhanced), \
        f"Expected decimal section in: {enhanced}"
    
    # Also test explicit decimal
    enhanced2 = DocumentRetriever._enhance_query_for_keywords("section 3.2")
    assert "section 3.2" in enhanced2 or "sec 3.2" in enhanced2, \
        f"Expected section variant in: {enhanced2}"
    
    return {"decimal_handling": "OK"}


def test_query_enhancement_figure_variants():
    """Test that figure references get all variants."""
    from rag.retriever import DocumentRetriever
    
    enhanced = DocumentRetriever._enhance_query_for_keywords("figure 37 success rate")
    
    assert "fig 37" in enhanced, f"Expected 'fig 37' in: {enhanced}"
    assert "fig. 37" in enhanced, f"Expected 'fig. 37' in: {enhanced}"
    assert "figure 37" in enhanced, f"Expected 'figure 37' in: {enhanced}"
    
    return {"variants_generated": "fig, fig., figure"}


# =============================================================================
# UNIT TESTS: Anchor Detection
# =============================================================================

def test_anchor_detection_figures():
    """Test figure anchor detection."""
    from rag.retriever import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    queries = [
        "What is in Figure 37?",
        "fig 37 shows what?",
        "Fig. 37 results",
    ]
    
    for query in queries:
        anchors = retriever._detect_anchors(query)
        assert "figure" in anchors, f"Expected figure anchor in: {query}"
        assert "37" in anchors["figure"], f"Expected '37' in figure anchors for: {query}"
    
    return {"queries_tested": len(queries)}


def test_anchor_detection_tables():
    """Test table anchor detection."""
    from rag.retriever import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    anchors = retriever._detect_anchors("What does Table 2.3 show?")
    assert "table" in anchors, "Expected table anchor"
    assert "2.3" in anchors["table"], "Expected '2.3' in table anchors"
    
    return {"table_detection": "OK"}


def test_anchor_detection_sections():
    """Test section anchor detection."""
    from rag.retriever import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    test_cases = [
        ("Section 4.1 discusses", "section", "4.1"),
        ("In sec 3.2 we see", "section", "3.2"),
        ("Chapter 5 analysis", "chapter", "5"),
    ]
    
    for query, anchor_type, expected_val in test_cases:
        anchors = retriever._detect_anchors(query)
        assert anchor_type in anchors, f"Expected {anchor_type} anchor in: {query}"
        assert expected_val in anchors[anchor_type], \
            f"Expected '{expected_val}' in {anchor_type} anchors for: {query}"
    
    return {"section_patterns_tested": len(test_cases)}


def test_anchor_detection_references():
    """Test reference bracket detection."""
    from rag.retriever import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    anchors = retriever._detect_anchors("Reference [15] and [23] discuss this")
    assert "reference" in anchors, "Expected reference anchor"
    assert "15" in anchors["reference"], "Expected '15' in references"
    assert "23" in anchors["reference"], "Expected '23' in references"
    
    return {"references_detected": 2}


# =============================================================================
# UNIT TESTS: Anchor Extraction (Indexer)
# =============================================================================

def test_anchor_extraction_from_text():
    """Test anchor string extraction during indexing."""
    from rag.indexer import DocumentIndexer
    
    text = """
    Figure 37 shows the success rate of 85%.
    Table 2 presents the configuration.
    See Section 4.1 for details.
    Reference [15] discusses this topic.
    """
    
    anchors = DocumentIndexer._extract_anchors(text)
    
    assert "figure 37" in anchors, f"Expected 'figure 37' in anchors: {anchors}"
    assert "table 2" in anchors, f"Expected 'table 2' in anchors: {anchors}"
    assert "section 4.1" in anchors, f"Expected 'section 4.1' in anchors: {anchors}"
    assert "reference 15" in anchors, f"Expected 'reference 15' in anchors: {anchors}"
    
    return {"anchors_extracted": len(anchors)}


# =============================================================================
# INTEGRATION TESTS: Full Retrieval Pipeline
# =============================================================================

def test_ingest_research_paper():
    """Ingest the test research paper for subsequent tests."""
    from rag import DocumentIndexer
    
    indexer = DocumentIndexer()
    
    # Delete if exists (clean state)
    indexer.delete_document("test-research-paper")
    
    result = indexer.ingest_text(
        text=RESEARCH_PAPER_TEXT,
        title="Video Models Zero-Shot Learners",
        document_id="test-research-paper",
    )
    
    assert result["success"], f"Ingestion failed: {result.get('error')}"
    assert result["num_chunks"] > 0, "No chunks created"
    
    return {
        "chunks_created": result["num_chunks"],
        "document_id": result["document_id"],
    }


def test_figure_37_retrieval():
    """
    Critical test: "What is the success rate in Figure 37?"
    
    This is the exact style of question mentioned in the interview spec.
    """
    from rag import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    # Test with exact reference - use higher k and lower threshold to get more chunks
    result = retriever.hybrid_retrieve(
        "What is the success rate in Figure 37?",
        document_id="test-research-paper",
        k=20,
        score_threshold=0.0,  # Lower threshold to include more results
    )
    
    assert len(result.documents) > 0, "No documents retrieved for Figure 37 query"
    
    # Combine all document content for checking
    all_content = " ".join(doc.page_content for doc in result.documents).lower()
    
    # Must find Figure 37 content in ANY of the returned chunks
    has_figure = "figure 37" in all_content or "fig 37" in all_content or "fig. 37" in all_content
    assert has_figure, f"Figure 37 not found in {len(result.documents)} chunks. Context: {result.context[:500]}..."
    
    # Should contain the success rate numbers somewhere in the results
    has_accuracy = "85%" in all_content or "94%" in all_content or "97%" in all_content
    assert has_accuracy, f"Success rate percentages not found in {len(result.documents)} chunks"
    
    return {
        "docs_retrieved": len(result.documents),
        "confidence": round(max(result.scores) if result.scores else 0, 3),
        "found_figure_37": has_figure,
        "found_accuracy": has_accuracy,
    }


def test_figure_37_voice_style():
    """Test voice-style query: "figure thirty seven" instead of "Figure 37"."""
    from rag import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    result = retriever.hybrid_retrieve(
        "What's in figure thirty seven about text manipulation success?",
        document_id="test-research-paper",
        k=10,
    )
    
    assert len(result.documents) > 0, "No documents for voice-style Figure 37 query"
    
    context_lower = result.context.lower()
    assert "success" in context_lower or "accuracy" in context_lower, \
        f"Success rate info not found for voice query: {result.context[:300]}..."
    
    return {"voice_query": "OK"}


def test_section_specific_retrieval():
    """Test section-specific queries."""
    from rag import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    result = retriever.hybrid_retrieve(
        "What does Section 4.2 say about text manipulation?",
        document_id="test-research-paper",
        k=10,
    )
    
    assert len(result.documents) > 0, "No documents for Section 4.2 query"
    
    context_lower = result.context.lower()
    assert "text manipulation" in context_lower or "success rate" in context_lower, \
        f"Section 4.2 content not found: {result.context[:300]}..."
    
    return {"section_query": "OK"}


def test_reference_retrieval():
    """Test reference bracket queries."""
    from rag import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    result = retriever.hybrid_retrieve(
        "What does reference 15 discuss?",
        document_id="test-research-paper",
        k=10,
    )
    
    assert len(result.documents) > 0, "No documents for reference query"
    
    context_lower = result.context.lower()
    # Reference [15] discusses video understanding and robotics
    has_ref_content = "[15]" in result.context or "reference 15" in context_lower
    assert has_ref_content, f"Reference [15] content not found: {result.context[:300]}..."
    
    return {"reference_query": "OK"}


def test_table_retrieval():
    """Test table-specific queries."""
    from rag import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    result = retriever.hybrid_retrieve(
        "What does Table 1 show about model configurations?",
        document_id="test-research-paper",
        k=10,
    )
    
    assert len(result.documents) > 0, "No documents for Table 1 query"
    
    context_lower = result.context.lower()
    assert "table 1" in context_lower or "config" in context_lower, \
        f"Table 1 content not found: {result.context[:300]}..."
    
    return {"table_query": "OK"}


def test_chapter_retrieval():
    """Test chapter-style queries (Section 5 = Chapter 5 in this paper)."""
    from rag import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    result = retriever.hybrid_retrieve(
        "What does Section 5 say about ablation studies?",
        document_id="test-research-paper",
        k=10,
    )
    
    assert len(result.documents) > 0, "No documents for Section 5 query"
    
    context_lower = result.context.lower()
    assert "ablation" in context_lower or "improvement" in context_lower, \
        f"Ablation study content not found: {result.context[:300]}..."
    
    return {"chapter_query": "OK"}


def test_structured_citations():
    """Test that citations include page/section/anchor info."""
    from rag import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    result = retriever.hybrid_retrieve(
        "Figure 37 success rate",
        document_id="test-research-paper",
        k=4,
    )
    
    assert len(result.documents) > 0, "No documents retrieved"
    
    # Check context has structured citation format
    has_source_tag = "[Source" in result.context or "[source" in result.context.lower()
    has_section_info = "Section" in result.context or "/" in result.context
    
    assert has_source_tag, f"Missing source tags in context: {result.context[:200]}..."
    
    return {"structured_citations": "OK"}


def test_reranking_improves_precision():
    """Test that re-ranking brings the most relevant document to the top."""
    from rag import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    # Query specifically about Figure 37's accuracy numbers
    result = retriever.hybrid_retrieve(
        "What is the exact accuracy percentage shown in Figure 37?",
        document_id="test-research-paper",
        use_reranking=True,
        k=15,
    )
    
    assert len(result.documents) > 0, "No documents retrieved"
    
    # The top result should contain Figure 37 + accuracy info
    top_doc = result.documents[0].page_content.lower()
    has_figure = "figure 37" in top_doc or "fig" in top_doc
    has_accuracy = "85%" in top_doc or "94%" in top_doc or "97%" in top_doc or "accuracy" in top_doc
    
    # At least one should be true after re-ranking
    assert has_figure or has_accuracy, \
        f"Top result doesn't contain Figure 37 content after re-ranking: {top_doc[:200]}..."
    
    return {
        "reranking": "enabled" if retriever._reranker else "disabled",
        "top_score": round(result.scores[0], 3) if result.scores else 0,
    }


def test_confidence_gating():
    """Test confidence gating for low-confidence results."""
    from rag import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    # Query for something NOT in the document
    result = retriever.hybrid_retrieve(
        "What is the weather forecast for tomorrow?",
        document_id="test-research-paper",
        k=5,
    )
    
    # Should return results but with low confidence
    # The confidence should be lower than for valid queries
    if result.scores:
        max_score = max(result.scores)
        # Low confidence expected for irrelevant query
        # (threshold depends on config but should generally be < 0.5)
        logger.info(f"Irrelevant query confidence: {max_score:.3f}")
    
    return {"confidence_check": "OK"}


def test_hybrid_vs_semantic_for_specific_refs():
    """Compare hybrid vs semantic-only for specific reference queries."""
    from rag import DocumentRetriever
    
    retriever = DocumentRetriever()
    
    query = "Figure 37 text manipulation"
    
    # Semantic-only - get more results
    semantic_result = retriever.retrieve(
        query,
        document_id="test-research-paper",
        k=10,
        score_threshold=0.0,
    )
    
    # Hybrid (semantic + BM25) - get more results
    hybrid_result = retriever.hybrid_retrieve(
        query,
        document_id="test-research-paper",
        k=20,
        score_threshold=0.0,
    )
    
    # Check across ALL returned documents, not just the context string
    hybrid_all = " ".join(doc.page_content.lower() for doc in hybrid_result.documents)
    semantic_all = " ".join(doc.page_content.lower() for doc in semantic_result.documents)
    
    hybrid_has_fig37 = "figure 37" in hybrid_all or "fig 37" in hybrid_all
    semantic_has_fig37 = "figure 37" in semantic_all or "fig 37" in semantic_all
    
    logger.info(f"Hybrid found Figure 37: {hybrid_has_fig37} ({len(hybrid_result.documents)} docs)")
    logger.info(f"Semantic found Figure 37: {semantic_has_fig37} ({len(semantic_result.documents)} docs)")
    
    # At least one method should find Figure 37
    # (In practice, hybrid should be better for specific refs, but both should work)
    found_by_at_least_one = hybrid_has_fig37 or semantic_has_fig37
    assert found_by_at_least_one, \
        f"Neither hybrid nor semantic found Figure 37 in {len(hybrid_result.documents)}/{len(semantic_result.documents)} docs"
    
    return {
        "hybrid_found_fig37": hybrid_has_fig37,
        "semantic_found_fig37": semantic_has_fig37,
        "hybrid_docs": len(hybrid_result.documents),
        "semantic_docs": len(semantic_result.documents),
    }


# =============================================================================
# CLEANUP
# =============================================================================

def cleanup_test_documents():
    """Remove test documents."""
    from rag import DocumentIndexer
    
    indexer = DocumentIndexer()
    indexer.delete_document("test-research-paper")
    
    return {"cleanup": "OK"}


# =============================================================================
# MAIN TEST RUNNER
# =============================================================================

def run_all_tests():
    """Run all specific fact retrieval tests."""
    print("\n" + "="*70)
    print("ðŸŽ¯ SPECIFIC FACT IN SPECIFIC CHAPTER - RETRIEVAL TESTS")
    print("   Testing the critical Bluejay interview requirement")
    print("="*70 + "\n")
    
    runner = TestRunner()
    
    # Unit tests (no ingestion required)
    print("\nðŸ“ UNIT TESTS: Query Enhancement")
    print("-"*50)
    runner.run_test("Compound Numbers", test_query_enhancement_compound_numbers)
    runner.run_test("Section Decimals", test_query_enhancement_section_decimals)
    runner.run_test("Figure Variants", test_query_enhancement_figure_variants)
    
    print("\nðŸ” UNIT TESTS: Anchor Detection")
    print("-"*50)
    runner.run_test("Figure Anchors", test_anchor_detection_figures)
    runner.run_test("Table Anchors", test_anchor_detection_tables)
    runner.run_test("Section Anchors", test_anchor_detection_sections)
    runner.run_test("Reference Anchors", test_anchor_detection_references)
    
    print("\nðŸ“„ UNIT TESTS: Anchor Extraction (Indexer)")
    print("-"*50)
    runner.run_test("Extract Anchors from Text", test_anchor_extraction_from_text)
    
    # Integration tests (require API key and ingestion)
    if os.getenv("OPENAI_API_KEY"):
        print("\nðŸ”— INTEGRATION TESTS: Full Pipeline")
        print("-"*50)
        runner.run_test("Ingest Research Paper", test_ingest_research_paper)
        runner.run_test("Figure 37 Retrieval (Critical)", test_figure_37_retrieval)
        runner.run_test("Figure 37 Voice-Style", test_figure_37_voice_style)
        runner.run_test("Section-Specific Query", test_section_specific_retrieval)
        runner.run_test("Reference Query", test_reference_retrieval)
        runner.run_test("Table Query", test_table_retrieval)
        runner.run_test("Chapter Query", test_chapter_retrieval)
        runner.run_test("Structured Citations", test_structured_citations)
        runner.run_test("Re-ranking Precision", test_reranking_improves_precision)
        runner.run_test("Confidence Gating", test_confidence_gating)
        runner.run_test("Hybrid vs Semantic", test_hybrid_vs_semantic_for_specific_refs)
        
        print("\nðŸ§¹ CLEANUP")
        print("-"*50)
        runner.run_test("Cleanup Test Documents", cleanup_test_documents)
    else:
        print("\nâš ï¸  Skipping integration tests (OPENAI_API_KEY not set)")
    
    print(runner.summary())
    
    # Return exit code
    failed = sum(1 for r in runner.results if not r.passed)
    return failed


if __name__ == "__main__":
    exit_code = run_all_tests()
    sys.exit(exit_code)

